{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Baseline test used random forest classifier to get the baseline classification results when using a) target data as training data, and b) source data+target data as training data.  \n",
    "\n",
    "The following ipython notebook (compatible with python 2.7+, need modifications for python 3) needs the following input files in the specified input format to get the baseline performance:\n",
    "\n",
    "-feature_file: \n",
    "A comma separated file which contains corresponding numbering and names of the extracted features from the light curve in the data file. The first column denotes the numbering of each feature and the second column denotes the name of each feature. 'config.dat' is the feature file we used for this research.\n",
    "\n",
    "-selected_features:\n",
    "A list which contains the numbering of the features we selected for this research\n",
    "\n",
    "-Class_list:\n",
    "A tab separated file which contains all class types appeared in the data file. The first column denotes the name of each class type and the fourth column denotes the numbering of each class type, which are correspondent to the last column of each data file. For example, if one row has 'EW' in the first column and 4 in the fourth column in the file Class_list, an object with class type 1 is an 'EW' object. \n",
    "\n",
    "-Selected_Class_names:\n",
    "A list which contains the names of selected class types we used for this research\n",
    "\n",
    "-Data files\n",
    "In this research, we used csdr2('CSDR2_lc_data.csv'), ptfr('R_PTF_lc_features.csv'), and lineardb('Linear_lc_over40.csv') as our datasets. Each of the dataset has the following format:\n",
    "\n",
    "The headerline includes the names of all features associated with this dataset along with the class type for each object;\n",
    "\n",
    "Each line (except the header) denotes one data entry and contains the information of all features for one object;\n",
    "\n",
    "The last column of each dataset denotes the class types for each object, the numbering of the class types should coresspond to the indices in Class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy.stats import sem\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.pyplot import setp\n",
    "from __future__ import division\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans \n",
    "from collections import Counter\n",
    "from unbalanced_dataset import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from itertools import compress\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details about the baseline test\n",
    "* no sampling methods, neither SMOTE nor under as we have ample data\n",
    "* select 400~500 from each class from each survey to form the domain data\n",
    "* eleminate part of the source data in training which has the same ID as the cross-validation data\n",
    "* save 10% objects from each class for testing\n",
    "* Use K-fold cross-validation to test for base results\n",
    "* plot with error bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creat feature labels for data\n",
    "def add_Names(filename, selected_features):\n",
    "    # Start with an empty dictionary\n",
    "    stats = {}\n",
    "    for line in open(filename):\n",
    "        # Split the feature file with delimiter ','; key is the feature number and value is feature name\n",
    "        line = line.replace(\"'\", \"\")\n",
    "        temp = line.rstrip().split(',')\n",
    "        stats[temp[0]] = temp[1]\n",
    "    features = []\n",
    "    for feature in selected_features:\n",
    "        features.append(stats[str(feature)])\n",
    "    features.append(class_label)\n",
    "    return features\n",
    "\n",
    "#Select data based on objects' labels (subset for certain classes)\n",
    "def selectdata(data, Selected_Class):\n",
    "    dataF = pd.DataFrame()\n",
    "    i = 0\n",
    "    for c in Selected_Class:\n",
    "        dataF =  dataF.append(data[data[class_label] == c])\n",
    "        #dataF.loc[data[class_label] == c,class_label] = i\n",
    "        i+=1\n",
    "    return dataF\n",
    "\n",
    "def test_train_label(X, split, offset):\n",
    "    len_feature = len(X.columns) - offset\n",
    "    train, test = cv.train_test_split(X, train_size=(split/100))\n",
    "    X_train = train.iloc[:,range(0,len_feature)]\n",
    "    y_train= train[class_label]\n",
    "    X_test = test.iloc[:,range(0,len_feature)]\n",
    "    y_test = test[class_label]\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "def normalize_data_with_label(data):\n",
    "    df = data.iloc[:,range(0,len_feature)]\n",
    "    data_norm = (df - df.mean()) / (df.max() - df.min())\n",
    "    data_norm[class_label] = data[class_label]\n",
    "    return data_norm\n",
    "\n",
    "#minus mean and divided by standard deviation\n",
    "def normalize_data_with_label2(data):\n",
    "    df = data.iloc[:,range(0,len_feature)]\n",
    "    data_norm = (df - df.mean()) / df.std()\n",
    "    data_norm[class_label] = data[class_label]\n",
    "    return data_norm\n",
    "\n",
    "def sample_wo_replacement(data, size, Selected_Class):\n",
    "    o_data = pd.DataFrame()\n",
    "    for c in Selected_Class:\n",
    "        temp = data[data[class_label] == c]\n",
    "        class_size = len(temp)\n",
    "        if size > class_size:\n",
    "            indexes = np.random.choice(temp.index, size-class_size, replace=True)\n",
    "            temp = temp.append(temp.ix[indexes])\n",
    "        else:\n",
    "            indexes = np.random.choice(temp.index, size, replace=False)\n",
    "            temp = temp.ix[indexes]\n",
    "        o_data = o_data.append(temp)\n",
    "    return o_data\n",
    "\n",
    "def sample_wo_replacement_by_ratio(data, ratio, Selected_Class):\n",
    "    o_data = pd.DataFrame()\n",
    "    for c in Selected_Class:\n",
    "        temp = data[data[class_label] == c]\n",
    "        class_size = len(temp)    \n",
    "        if ratio > 1:\n",
    "            indexes = np.random.choice(temp.index, class_size*(ratio-1), replace=True)\n",
    "            temp = temp.append(temp.ix[indexes])\n",
    "        else:\n",
    "            indexes = np.random.choice(temp.index, class_size*ratio, replace=False)\n",
    "            temp = temp.ix[indexes]\n",
    "        o_data = o_data.append(temp)\n",
    "    return o_data\n",
    "\n",
    "def findSmallestClassSize(data, Selected_Class):\n",
    "    smallest_size = 9999999999\n",
    "    for c in Selected_Class:\n",
    "        temp = data[data[class_label] == c]\n",
    "        class_size = len(temp)\n",
    "        if smallest_size > class_size:\n",
    "            smallest_size = class_size\n",
    "    return smallest_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Domain_Adaptation23_modified(S_data, T_data, Clf, target_smaller_class, source_smaller_class, checkBoth):\n",
    "    Learning_accuracies = []\n",
    "    Feature_importance = []\n",
    "    Learning_accuracies_TT = []\n",
    "    Feature_importance_TT = []\n",
    "    #select n number of data from each class in target domain for cross-validation\n",
    "    Y_tests = []\n",
    "    Y_preds = []\n",
    "    Y_preds_TT = []\n",
    "    Y_indexes = []\n",
    "    for split in range(start_size,end_size,bins):\n",
    "        Y_test = []\n",
    "        Y_pred = []\n",
    "        Y_pred_TT = []\n",
    "        Y_index = []\n",
    "        learn_accuracy = []\n",
    "        learn_accuracy_TT = []\n",
    "        importances = []\n",
    "        importances_TT = []\n",
    "        #select size objects from each class for training\n",
    "        size = ((split/100)*target_smaller_class)\n",
    "        for i in range(0, 10):\n",
    "            Source_train = sample_wo_replacement(S_data, source_smaller_class, Selected_Class)\n",
    "            #perform iter_time1 selecting Target_train\n",
    "            for j in range(0,iter_time1):\n",
    "                scores = []\n",
    "                scores_TT = []\n",
    "                #size: the size of a single class, so in total, target data for training will be size*num_of_classes\n",
    "                Target_train = sample_wo_replacement(T_data, size, Selected_Class)         \n",
    "                #perform iter_time2 slecting Test_target\n",
    "                for k in range(0,iter_time2):\n",
    "                    #include objects that are in the class\n",
    "                    Test_target = (T_data[~T_data.index.isin(Target_train.index.values)]).sample(frac=0.1)\n",
    "                    X_test = np.array(Test_target.ix[:,range(0,len_feature)])\n",
    "                    y_test = Test_target[class_label].values\n",
    "                    y_index = Test_target.index.values\n",
    "                    \n",
    "                    Train_data = Target_train.append(Source_train)\n",
    "                    X_train = np.array(Train_data.ix[:,range(0,len_feature)])\n",
    "                    y_train = Train_data[class_label].values\n",
    "                    Clf.fit(X_train, y_train)\n",
    "                    scores.append(Clf.score(X_test, y_test))\n",
    "                    Y_test.extend(y_test)\n",
    "                    Y_pred.extend(Clf.predict(X_test))\n",
    "                    Y_index.extend(y_index)\n",
    "                    if (type(Clf) == type(rfc)):\n",
    "                        importances.append(Clf.feature_importances_)\n",
    "                    if checkBoth:\n",
    "                        X_train = np.array(Target_train.ix[:,range(0,len_feature)])\n",
    "                        y_train = Target_train[class_label].values\n",
    "                        #y_train = y_train.astype(int)\n",
    "                        Clf.fit(X_train, y_train)\n",
    "                        scores_TT.append(Clf.score(X_test, y_test))\n",
    "                        Y_pred_TT.extend(Clf.predict(X_test))\n",
    "                        if (type(Clf) == type(rfc)):\n",
    "                            importances_TT.append(Clf.feature_importances_)\n",
    "                learn_accuracy.append(np.mean(scores))\n",
    "                learn_accuracy_TT.append(np.mean(scores_TT))\n",
    "        if (type(Clf) == type(rfc)):\n",
    "            ave_f_importance =  np.mean(np.array(importances), axis=0)\n",
    "            Feature_importance.append(ave_f_importance)\n",
    "            ave_f_importance_TT =  np.mean(np.array(importances_TT), axis=0)\n",
    "            Feature_importance_TT.append(ave_f_importance_TT)\n",
    "        Learning_accuracies.append(np.mean(learn_accuracy))\n",
    "        Y_tests.append(Y_test)\n",
    "        Learning_accuracies_TT.append(np.mean(learn_accuracy_TT))\n",
    "        Y_preds_TT.append(Y_pred_TT)\n",
    "        Y_preds.append(Y_pred)\n",
    "        Y_indexes.append(Y_index)\n",
    "    return Learning_accuracies, Learning_accuracies_TT, Clf, Feature_importance, Feature_importance_TT, Y_tests, Y_preds, Y_preds_TT, Y_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_size = 5 #starting ratio that denotes the % of the target data used as training set\n",
    "end_size = 50 #ending ratio that denotes the % of the target data used as training set\n",
    "bins = 5 \n",
    "runs = 4 #number of runs for each baseline test\n",
    "iter_time1 = 10 #number of iterations where each iteration samples a balanced training set from the target domain\n",
    "iter_time2 = 10 #number of iterations where each iteration samples a testing set from the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating feature labels\n",
    "feature_file = 'config.dat'\n",
    "selected_features = range(1,22)\n",
    "selected_features.remove(15)\n",
    "len_feature = len(selected_features)\n",
    "labels = add_Names(feature_file,selected_features)\n",
    "\n",
    "#creating selected class types\n",
    "class_label = \"class\"\n",
    "classes = {}\n",
    "Class_list = 'Class_list'\n",
    "for line in open(Class_list):\n",
    "    # Split the config.dat file with delimiter ','; key is the feature number and value is feature name\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    temp = line.rstrip().split('\\t')\n",
    "    classes[temp[0]] = temp[3]\n",
    "Selected_Class_names = [\"EW\",\"EA\",\"RRab\",\"RRc\",\"RRd\",\"RS CVn\"]  \n",
    "Selected_Class = [int(classes[x]) for x in Selected_Class_names]\n",
    "\n",
    "rfc = RandomForestClassifier(class_weight='auto')\n",
    "clf = rfc\n",
    "\n",
    "#Reading data\n",
    "csdr2 = pd.read_csv('CSDR2_lc_data.csv')\n",
    "ptfr = pd.read_csv('R_PTF_lc_features.csv')\n",
    "lineardb = pd.read_csv('Linear_lc_over40.csv')\n",
    "test_smaller_class = len(ptfr[ptfr[class_label]==6]) \n",
    "crts = csdr2\n",
    "\n",
    "'''Domain Adaptation learning curve for T-2-T and S+T-2-T'''\n",
    "Learning_curves = []\n",
    "labels = []\n",
    "Y_Tests = []\n",
    "Y_Preds_TT = []\n",
    "Y_Preds = []\n",
    "Y_indexes = []\n",
    "for run in range(0,runs):\n",
    "    print \"run %d\" %run\n",
    "    Learning_curve = []\n",
    "    for S_data, T_data, s_name, t_name, checkBoth in [(lineardb, crts, \"Lineardb\", \"CRTS\", True),\n",
    "                                                      (crts, ptfr, \"CRTS\", \"PTF(r)\", True),\n",
    "                                                      (ptfr, crts, \"PTF(r)\", \"CRTS\", False),\n",
    "                                                      (crts, lineardb, \"CRTS\", \"Lineardb\", True), \n",
    "                                                      (ptfr, lineardb, \"PTF(r)\", \"Lineardb\", False), \n",
    "                                                      (lineardb, ptfr, \"Lineardb\", \"PTF(r)\",False)]:\n",
    "        S_data = normalize_data_with_label2(S_data)\n",
    "        T_data = normalize_data_with_label2(T_data)\n",
    "        target_smaller_class = findSmallestClassSize(T_data, Selected_Class)\n",
    "        source_smaller_class = findSmallestClassSize(S_data, Selected_Class)\n",
    "        Learning_accuracies, Learning_accuracies_TT, Clf, Feature_importance, Feature_importance_TT, Y_tests, Y_preds, Y_preds_TT, Y_index = Domain_Adaptation23_modified(\n",
    "            S_data, T_data, clf, target_smaller_class, source_smaller_class, checkBoth)\n",
    "        pickle.dump([Learning_accuracies, Feature_importance], open(\"S+T to T %s-2-%s_run%d.p\" % (s_name,t_name, run), \"w\" ))\n",
    "        Learning_curve.append(Learning_accuracies)\n",
    "        labels.append(\"S+T to T %s-2-%s\" % (s_name,t_name))\n",
    "        \n",
    "        if checkBoth:\n",
    "            Learning_curve.append(Learning_accuracies_TT)\n",
    "            labels.append(\"T to T %s\" % t_name)\n",
    "            Y_Preds_TT.append(Y_preds_TT)\n",
    "            pickle.dump([Learning_accuracies_TT, Feature_importance_TT], open(\"T to T %s_run%d.p\" % (t_name,run), \"w\" ))\n",
    "        Y_Tests.append(Y_tests)\n",
    "        Y_Preds.append(Y_preds)\n",
    "        Y_indexes.append(Y_index)   \n",
    "    Learning_curves.append(np.array(Learning_curve))\n",
    "fig = plt.figure(figsize=(18.75,10))\n",
    "ax = fig.add_subplot(111)\n",
    "curve_plot = np.mean(Learning_curves,axis=0)\n",
    "pickle.dump([Learning_curves, labels], open(\"Learning_curves_w_labels.p\", \"w\" ))\n",
    "std_err = sem(Learning_curves)\n",
    "for index in range(0, curve_plot.shape[0]):\n",
    "    #ax.plot(range(start_size,end_size,bins), Learning_accuracies, \"s-\",\n",
    "    #            label=labels[index],marker = Line2D.filled_markers[index])\n",
    "    ax.errorbar(range(start_size,end_size,bins), curve_plot[index], yerr=std_err[index], \n",
    "                label=labels[index], marker = Line2D.filled_markers[index])\n",
    "\n",
    "ax.set_title(\"Average acurracies for S+T-to-T and T-to-T (three surveys)\",fontsize=24)\n",
    "ax.set_xlabel(\"Target data training percentage %\",fontsize=18)\n",
    "ax.set_ylabel(\"Average accuracy\",fontsize=18)\n",
    "\n",
    "# Shrink current axis by 20%\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "ax.tick_params(axis='x', labelsize=18)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 0.6),fontsize=18)\n",
    "plt.savefig('three_surveys_RF_DA2_3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------K_fold_cross_validation baseline tests for in-domain accuracy---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def K_fold_cross_validation(K_fold, crts, ptfr, lineardb):\n",
    "    Y_Tests = []\n",
    "    Y_Preds = []\n",
    "    print \"%d_fold cross validation: \" %K_fold\n",
    "    for data, name in [(crts,\"crts\"), (ptfr,\"ptfr\"), (lineardb,\"lineardb\")]:\n",
    "        data = normalize_data_with_label2(data)\n",
    "        Y_test = []\n",
    "        Y_pred = []\n",
    "        Indexes = []\n",
    "        test_smaller_class = len(data[data[class_label]==6])\n",
    "        for c in Selected_Class:\n",
    "            temp =  data[data[class_label] == c]\n",
    "            len_size = test_smaller_class//K_fold #len(temp) or to get a balanced data: test_smaller_class \n",
    "            chunks = []\n",
    "            for i in range(0, K_fold):   \n",
    "                rows = temp.sample(len_size).index.values\n",
    "                chunks.append(rows)\n",
    "                temp = temp.drop(rows)\n",
    "            Indexes.append(chunks)\n",
    "\n",
    "        #Every row in Indexes is the data for class c that are splited into K chuncks\n",
    "        class_index = []\n",
    "        for i in range(0, K_fold):\n",
    "            indexes_split = []\n",
    "            for c in range(0,len(Indexes)):\n",
    "                indexes_split.extend(Indexes[c][i])\n",
    "            class_index.append(indexes_split)\n",
    "\n",
    "        #K_fold cross validation\n",
    "        scores = []\n",
    "        for class_for_test in range(0, K_fold):\n",
    "            temp_index = []\n",
    "            for i in range(0, K_fold):\n",
    "                if(i is not class_for_test):\n",
    "                    temp_index.extend(class_index[i])\n",
    "            train = data[data.index.isin(temp_index)]\n",
    "            test = data[data.index.isin(class_index[class_for_test])]\n",
    "            rfc.fit(train.iloc[:,range(0,len_feature)], train[class_label])\n",
    "            Y_pred.extend(rfc.predict(test.iloc[:,range(0,len_feature)]))\n",
    "            Y_test.extend(test[class_label])        \n",
    "            scores.append(rfc.score(test.iloc[:,range(0,len_feature)], test[class_label]))\n",
    "        print \"average accuracy for %s is %f\" % (name,np.mean(scores))\n",
    "        Y_Tests.append(Y_test)       \n",
    "        Y_Preds.append(Y_pred)\n",
    "    return Y_Tests, Y_Preds\n",
    "\n",
    "def plot_confusion_matrix(cm, title, Selected_Class, cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(Selected_Class))\n",
    "    plt.xticks(tick_marks, Selected_Class, rotation=45)\n",
    "    plt.yticks(tick_marks, Selected_Class)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "'''\n",
    "names = ['crts','ptfr','lineardb']\n",
    "K = 2\n",
    "Y_Test, Y_Pred = K_fold_cross_validation(K, crts, ptfr, lineardb)\n",
    "for i in range(len(Y_Test)):\n",
    "    cm = confusion_matrix(Y_Test[i], Y_Pred[i])\n",
    "    np.set_printoptions(precision=3)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print('Normalized Confusion matrix for %s under %d-fold cv' %(names[i], K))\n",
    "    print(cm_normalized)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm_normalized, names[i], Selected_Class)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

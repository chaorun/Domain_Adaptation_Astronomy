{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared the classification results of four different methods (RandomForest with target data as training data,  RandomForest without target data as training data, and two domain adaptation techniques: GFK and CODA). Since the two methods CODA and GFK are written in Matlab, this program calls MATLAB and passed same training data in each iteration to get corresponding results.\n",
    "\n",
    "You need to configure your MATLAB location in order to use pymatbridge (need to install first, https://arokem.github.io/python-matlab-bridge/)\n",
    "\n",
    "Additionally, the following ipython notebook (compatible with python 2.7+, need modifications for python 3) needs the following input files in the specified input format to get the baseline performance:\n",
    "\n",
    "-feature_file: \n",
    "A comma separated file which contains corresponding numbering and names of the extracted features from the light curve in the data file. The first column denotes the numbering of each feature and the second column denotes the name of each feature. 'config.dat' is the feature file we used for this research.\n",
    "\n",
    "-selected_features:\n",
    "A list which contains the numbering of the features we selected for this research\n",
    "\n",
    "-Class_list:\n",
    "A tab separated file which contains all class types appeared in the data file. The first column denotes the name of each class type and the fourth column denotes the numbering of each class type, which are correspondent to the last column of each data file. For example, if one row has 'EW' in the first column and 4 in the fourth column in the file Class_list, an object with class type 1 is an 'EW' object. \n",
    "\n",
    "-Selected_Class_names:\n",
    "A list which contains the names of selected class types we used for this research\n",
    "\n",
    "-Data files\n",
    "In this research, we used csdr2('CSDR2_lc_data.csv'), ptfr('R_PTF_lc_features.csv'), and lineardb('Linear_lc_over40.csv') as our datasets. Each of the dataset has the following format:\n",
    "\n",
    "The headerline includes the names of all features associated with this dataset along with the class type for each object;\n",
    "\n",
    "Each line (except the header) denotes one data entry and contains the information of all features for one object;\n",
    "\n",
    "The last column of each dataset denotes the class types for each object, the numbering of the class types should coresspond to the indices in Class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "from pandas import DataFrame\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from pymatbridge import Matlab\n",
    "%matplotlib inline\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.matlib import repmat\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "from scipy.stats import sem\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creat labels for data\n",
    "def add_Names(filename, selected_features):\n",
    "    # Start with an empty dictionary\n",
    "    stats = {}\n",
    "    for line in open(filename):\n",
    "        # Split the config.dat file with delimiter ','; key is the feature number and value is feature name\n",
    "        line = line.replace(\"'\", \"\")\n",
    "        temp = line.rstrip().split(',')\n",
    "        stats[temp[0]] = temp[1]\n",
    "    features = []\n",
    "    for feature in selected_features:\n",
    "        features.append(stats[str(feature)])\n",
    "    features.append(class_label)\n",
    "    return features\n",
    "\n",
    "#Select data based on objects' labels (subset for certain classes)\n",
    "def selectdata(data, Selected_Class):\n",
    "    dataF = pd.DataFrame()\n",
    "    i = 0\n",
    "    for c in Selected_Class:\n",
    "        dataF =  dataF.append(data[data[class_label] == c])\n",
    "        #dataF.loc[data[class_label] == c,class_label] = i\n",
    "        i+=1\n",
    "    return dataF\n",
    "\n",
    "def normalize_data_with_label(data):\n",
    "    df = data.iloc[:,range(0,len_feature)]\n",
    "    data_norm = (df - df.mean()) / (df.max() - df.min())\n",
    "    data_norm[class_label] = data[class_label]\n",
    "    return data_norm\n",
    "\n",
    "#minus mean and divided by standard deviation\n",
    "def normalize_data_with_label2(data):\n",
    "    df = data.iloc[:,range(0,len_feature)]\n",
    "    data_norm = (df - df.mean()) / df.std()\n",
    "    data_norm[class_label] = data[class_label]\n",
    "    return data_norm\n",
    "\n",
    "def sample_wo_replacement(data, size, Selected_Class):\n",
    "    o_data = pd.DataFrame()\n",
    "    for c in Selected_Class:\n",
    "        temp = data[data[class_label] == c]\n",
    "        class_size = len(temp)\n",
    "        if size > class_size:\n",
    "            indexes = np.random.choice(temp.index, size-class_size, replace=True)\n",
    "            temp = temp.append(temp.ix[indexes])\n",
    "        else:\n",
    "            indexes = np.random.choice(temp.index, size, replace=False)\n",
    "            temp = temp.ix[indexes]\n",
    "        o_data = o_data.append(temp)\n",
    "    return o_data\n",
    "\n",
    "def sample_wo_replacement_by_ratio(data, ratio, Selected_Class):\n",
    "    o_data = pd.DataFrame()\n",
    "    for c in Selected_Class:\n",
    "        temp = data[data[class_label] == c]\n",
    "        class_size = len(temp)\n",
    "        if ratio > 1:\n",
    "            indexes = np.random.choice(temp.index, class_size*(ratio-1), replace=True)\n",
    "            temp = temp.append(temp.ix[indexes])\n",
    "        else:\n",
    "            indexes = np.random.choice(temp.index, class_size*ratio, replace=False)\n",
    "            temp = temp.ix[indexes]\n",
    "        o_data = o_data.append(temp)\n",
    "    return o_data\n",
    "\n",
    "def format_data(source_dec, target_dec, Selected_pair, max_iter_times=10, min_iter_times=2):\n",
    "    '''determine the size of the smaller class'''\n",
    "    \n",
    "    target_smaller_class  = 99999999\n",
    "    target_bigger_class  = -99999999\n",
    "    \n",
    "    source_smaller_class  = 99999999\n",
    "    source_bigger_class  = -99999999\n",
    "    \n",
    "    larger_c_target = -1\n",
    "    larger_c_source = -1\n",
    "    \n",
    "    for data, flag in [(source_dec, False), (target_dec, True)]:\n",
    "        newdata = pd.DataFrame()\n",
    "        i = 1\n",
    "        for c in Selected_pair:\n",
    "            selection = (data[class_label] == c)\n",
    "            temp = data[selection]\n",
    "            temp.loc[:,class_label] = i\n",
    "            newdata = newdata.append(temp)            \n",
    "            if flag:\n",
    "                if (len(temp) < target_smaller_class):\n",
    "                    target_smaller_class  = len(temp)\n",
    "                if (len(temp) > target_bigger_class):\n",
    "                    target_bigger_class  = len(temp)\n",
    "                    larger_c_target = i\n",
    "            if not flag:\n",
    "                if (len(temp) < source_smaller_class):\n",
    "                    source_smaller_class  = len(temp)\n",
    "                if (len(temp) > source_bigger_class):\n",
    "                    source_bigger_class  = len(temp)\n",
    "                    larger_c_source = i\n",
    "            i = i-2\n",
    "        if flag:\n",
    "            target_dec = newdata\n",
    "        else:\n",
    "            source_dec = newdata\n",
    "        \n",
    "    '''determine how many partitions shall we split the larger group'''\n",
    "    ratio_target = target_bigger_class//target_smaller_class\n",
    "    ratio_source = source_bigger_class//source_smaller_class\n",
    "    target_replace_flag = False\n",
    "    source_replace_flag = False\n",
    "    '''make sure that we at least have min_iter_times partitions and at most have max_iter_times'''\n",
    "    if ratio_target > max_iter_times:\n",
    "        ratio_target = max_iter_times\n",
    "    elif ratio_target < min_iter_times:\n",
    "        ratio_target = min_iter_times\n",
    "        target_replace_flag = True\n",
    "    if ratio_source > max_iter_times:\n",
    "        ratio_source = max_iter_times\n",
    "    elif ratio_source < min_iter_times:\n",
    "        ratio_source = min_iter_times\n",
    "        source_replace_flag = True\n",
    "    '''Partition target data'''\n",
    "    re_target = []\n",
    "    for k in range(ratio_target):\n",
    "        temp_target = sample_wo_replacement(target_dec, target_smaller_class, [1, -1])\n",
    "        if not target_replace_flag:\n",
    "            index_del_target = (temp_target[temp_target[class_label] == larger_c_target]).index.values\n",
    "            target_dec = target_dec[~target_dec.index.isin(index_del_target)]\n",
    "        re_target.append(temp_target)\n",
    "    '''Partition source data''' \n",
    "    re_source = []\n",
    "    for k in range(ratio_source):\n",
    "        temp_source = sample_wo_replacement(source_dec, source_smaller_class, [1, -1])\n",
    "        if not source_replace_flag:\n",
    "            index_del_source = (temp_source[temp_source[class_label] == larger_c_source]).index.values\n",
    "            source_dec = source_dec[~source_dec.index.isin(index_del_source)]\n",
    "        re_source.append(temp_source)\n",
    "    return re_source, re_target, target_smaller_class, ratio_target, ratio_source, larger_c_target\n",
    "\n",
    "'''formulate data for CODA'''\n",
    "def CODA(source, target, tr_ratio, un_ration, test_ratio, test_smaller_class, larger_c, mlab):\n",
    "    #Note! since Matlab index starts at 1, count always equal to len(dataX)+1\n",
    "    count = 1\n",
    "    Selected_Class = [1,-1]\n",
    "    dataLabSS = source\n",
    "    dataLabTT = sample_wo_replacement(target, test_smaller_class*tr_ratio, Selected_Class)\n",
    "    idxSS = range(count,(len(dataLabSS)+count))\n",
    "\n",
    "    temp = target[target.index.isin(dataLabTT.index.values)]\n",
    "    temp = temp[temp[class_label] == larger_c]\n",
    "    \n",
    "    dataX = dataLabSS.append(dataLabTT)\n",
    "    idxLabs = range(count,(len(dataX)+count))\n",
    "    count = len(dataX)+count\n",
    "\n",
    "    target_remain = target[~target.index.isin(dataLabTT.index.values)]\n",
    "    dataUnls = sample_wo_replacement(target_remain, test_smaller_class*un_ration, Selected_Class)\n",
    "    idxUnls = range(count,(len(dataUnls)+count))\n",
    "    count = len(dataUnls)+count\n",
    "    dataX = dataX.append(dataUnls)\n",
    "\n",
    "    target_remain = target_remain[~target_remain.index.isin(dataUnls.index.values)]\n",
    "    #use imbalanced test data\n",
    "    dataTest = sample_wo_replacement_by_ratio(target_remain, test_ratio, Selected_Class)\n",
    "    idxTest = range(count,(len(dataTest)+count))\n",
    "    count = len(dataTest)+count  \n",
    "    dataX = dataX.append(dataTest)\n",
    "    idxTT = range(len(dataLabSS)+1,len(dataX)+1)\n",
    "\n",
    "    labels = dataX[class_label].values\n",
    "    dataX_norm1 = normalize_data_with_label(dataX)\n",
    "    dataX_norm1 = np.array(dataX_norm1.iloc[:,range(0,len_feature)])\n",
    "    res = mlab.run_func('coda_setup_modified.m', {'arg1': dataX_norm1.T, 'arg2': labels, \n",
    "                               'arg3': idxLabs, 'arg4':idxUnls, 'arg5':idxTest, \n",
    "                               'arg6': idxSS, 'arg7': idxTT})\n",
    "    res\n",
    "    pred = (res['result'])[0]\n",
    "    acc_CODA = pred[len(pred)-1]\n",
    "    pred = pred[:(len(pred)-1)]\n",
    "    return acc_CODA, dataX_norm1, labels, idxLabs, idxUnls, idxTest, dataLabTT, pred\n",
    "\n",
    "def GFK_RF(dataX_norm1, labels, idxLabs, idxUnls, idxTest, dataLabTT, mlab):\n",
    "    dataLabTT = normalize_data_with_label(dataLabTT)\n",
    "    idxLabs = np.array(idxLabs) \n",
    "    idxTest = np.array(idxTest) \n",
    "    idxUnls = np.array(idxUnls)\n",
    "    Xs = dataX_norm1[idxLabs - 1]  \n",
    "    Ys = labels[idxLabs - 1]    \n",
    "    pca.fit(Xs)  \n",
    "    Ps = pca.components_# source subspace\n",
    "    Xt = dataX_norm1[idxTest - 1]\n",
    "    Yt = labels[idxTest - 1]             \n",
    "    pca.fit(Xt)  \n",
    "    Pt = pca.components_# target subspace\n",
    "\n",
    "    Ps = Ps.T\n",
    "    Pt = Pt.T\n",
    "    #print \"running gfk\"\n",
    "    res = mlab.run_func('GFK_modified.m', {'arg1': Ps, 'arg2': Pt[:,0:10]})\n",
    "    G = res['result']\n",
    "    #print \"end running gfk \", mlab_run_time\n",
    "    XsG = np.matrix(Xs)*np.matrix(G)\n",
    "    XtG = np.matrix(Xt)*np.matrix(G)\n",
    "    withG_scores = []\n",
    "    woG_scores = []\n",
    "    T2T_scores = []\n",
    "    target_tr_y = dataLabTT[class_label].values\n",
    "    target_tr_X = np.array(dataLabTT.iloc[:,range(0,len_feature)])\n",
    "    \n",
    "    Y_pred_GFK = []\n",
    "    Y_pred_RF = []\n",
    "    Y_pred_T2T = []\n",
    "    fimp_GFK = []\n",
    "    fimp_RF = []\n",
    "    fimp_T2T = []\n",
    "    for i in range(iter_time1):\n",
    "        clf.fit(XsG, Ys)\n",
    "        score = clf.score(XtG, Yt)\n",
    "        withG_scores.append(score)\n",
    "        Y_pred_GFK.extend(clf.predict(XtG))\n",
    "        fimp_GFK.append(clf.feature_importances_)\n",
    "        \n",
    "        clf.fit(Xs, Ys)\n",
    "        score = clf.score(Xt, Yt)\n",
    "        woG_scores.append(score)\n",
    "        Y_pred_RF.extend(clf.predict(Xt))\n",
    "        fimp_RF.append(clf.feature_importances_)\n",
    "        \n",
    "        clf.fit(target_tr_X, target_tr_y)\n",
    "        score = clf.score(Xt, Yt)\n",
    "        T2T_scores.append(score)\n",
    "        Y_pred_T2T.extend(clf.predict(Xt))\n",
    "        fimp_T2T.append(clf.feature_importances_)\n",
    "        \n",
    "    acc_GFK = np.mean(withG_scores)\n",
    "    acc_RF = np.mean(woG_scores)\n",
    "    acc_T2T = np.mean(T2T_scores)\n",
    "    return acc_GFK, acc_RF, acc_T2T, Y_pred_GFK, Y_pred_RF, Y_pred_T2T, np.mean(fimp_GFK, axis=0), np.mean(fimp_RF, axis=0), np.mean(fimp_T2T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_label = \"class\"\n",
    "feature_file = 'config.dat'\n",
    "selected_features = range(1,22)\n",
    "selected_features.remove(15)\n",
    "len_feature = len(selected_features)\n",
    "labels = add_Names(feature_file,selected_features)\n",
    "\n",
    "#creating selected class types\n",
    "classes = {}\n",
    "Class_list = 'Class_list'\n",
    "for line in open(Class_list):\n",
    "    # Split the config.dat file with delimiter ','; key is the feature number and value is feature name\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    temp = line.rstrip().split('\\t')\n",
    "    classes[temp[0]] = temp[3]\n",
    "Selected_Class_names = [\"EW\",\"EA\",\"RRab\",\"RRc\",\"RRd\",\"RS CVn\"]  \n",
    "Selected_Class = [int(classes[x]) for x in Selected_Class_names]\n",
    "\n",
    "rfc = RandomForestClassifier(class_weight='auto')\n",
    "pca = PCA()\n",
    "\n",
    "#Reading data\n",
    "csdr2 = pd.read_csv('CSDR2_lc_data.csv')\n",
    "ptfr = pd.read_csv('R_PTF_lc_features.csv')\n",
    "lineardb = pd.read_csv('Linear_lc_over40.csv')\n",
    "test_smaller_class = len(ptfr[ptfr[class_label]==6]) \n",
    "crts = csdr2\n",
    "\n",
    "#generate selected pairs (pairwise classification)\n",
    "Selected_pairs = []\n",
    "for i in range(len(Selected_Class)):\n",
    "    for j in range(i+1, len(Selected_Class)):\n",
    "        Selected_pairs.append([Selected_Class[i],Selected_Class[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_per_pair = []\n",
    "errorbars_per_pair = []\n",
    "Y_preds_pairs = []\n",
    "Y_tests_pairs = []\n",
    "feature_importance_per_pair = []\n",
    "mlab = Matlab(executable='/Applications/MATLAB_R2015a.app/bin/matlab')\n",
    "Selected_Class = [1, -1]\n",
    "tr_ratio = 0.1\n",
    "un_ration = 0.7\n",
    "test_ratio = 0.2\n",
    "iter_time1 = 4\n",
    "iter_time2 = 2\n",
    "clf = rfc\n",
    "mlab.start()\n",
    "mlab_run_time = 0\n",
    "\n",
    "for Selected_pair in Selected_pairs:\n",
    "    print Selected_pair\n",
    "    scores_per_DA = []\n",
    "    errorbars = []\n",
    "    \n",
    "    Y_preds_CODA = [] \n",
    "    Y_preds_GFK = [] \n",
    "    Y_preds_RF = [] \n",
    "    Y_preds_T2T = []\n",
    "    Y_tests = []\n",
    "    F_importance_GFK = []\n",
    "    F_importance_RF = []\n",
    "    F_importance_T2T = []\n",
    "    for source, target in [(crts, ptfr)]:\n",
    "        source = selectdata(source, Selected_pair)\n",
    "        target = selectdata(target, Selected_pair)\n",
    "        '''format source and target data, get num of smaller class, determine how many times shall we repeat (ratio)'''\n",
    "        source_set, target_set, target_smaller_class, ratio_target, ratio_source, larger_c_target = format_data(\n",
    "            source, target, Selected_pair)\n",
    "        scores_CODA = []\n",
    "        scores_GFK = []\n",
    "        scores_RF = []\n",
    "        scores_T2T = []\n",
    "        fimp_GFKs = []\n",
    "        fimp_RFs = []\n",
    "        fimp_T2Ts = []\n",
    "        for i in range(ratio_source):\n",
    "            source = source_set[i]\n",
    "            for j in range(ratio_target):\n",
    "                target = target_set[j]\n",
    "                '''Results for CODA'''\n",
    "                acc_CODA, dataX_norm1, labels, idxLabs, idxUnls, idxTest, \\\n",
    "                dataLabTT, Y_pred_CODA = CODA(\n",
    "                    source, target, tr_ratio, un_ration, test_ratio, target_smaller_class, \n",
    "                    larger_c_target, mlab)\n",
    "                mlab_run_time = mlab_run_time+1           \n",
    "                scores_CODA.append(acc_CODA)\n",
    "                print mlab_run_time\n",
    "\n",
    "                '''The results using the same data using GFK and random forest'''\n",
    "                acc_GFK, acc_RF, accT2T, Y_pred_GFK, Y_pred_RF, Y_pred_T2T, fimp_GFK, fimp_RF, fimp_T2T = GFK_RF(\n",
    "                    dataX_norm1, labels, idxLabs, idxUnls, idxTest, dataLabTT, mlab)\n",
    "                mlab_run_time = mlab_run_time+1\n",
    "                scores_GFK.append(acc_GFK)\n",
    "                scores_RF.append(acc_RF)\n",
    "                scores_T2T.append(accT2T)\n",
    "\n",
    "                fimp_GFKs.append(fimp_GFK)\n",
    "                fimp_RFs.append(fimp_RF)\n",
    "                fimp_T2Ts.append(fimp_T2T)\n",
    "                mlab.stop()\n",
    "                mlab.start()\n",
    "                errorbars.append([scores_CODA, scores_GFK, scores_RF, scores_T2T])\n",
    "                scores_per_DA.append([1-np.mean(scores_CODA), np.mean(scores_GFK), np.mean(scores_RF), np.mean(scores_T2T)])\n",
    "                F_importance_GFK.append(fimp_GFKs)\n",
    "                F_importance_RF.append(fimp_RFs)\n",
    "                F_importance_T2T.append(fimp_T2Ts)\n",
    "                \n",
    "            idxTest = np.array(idxTest) \n",
    "            Y_preds_GFK.extend(Y_pred_GFK)\n",
    "            Y_preds_RF.extend(Y_pred_RF)\n",
    "            Y_preds_T2T.extend(Y_pred_T2T)    \n",
    "            Y_test = labels[idxTest - 1]\n",
    "            for k in range(ratio_source):\n",
    "                Y_tests.extend(Y_test)\n",
    "                Y_preds_CODA.extend(Y_pred_CODA)\n",
    "    Y_preds_pairs.append([Y_preds_CODA, Y_preds_GFK, Y_preds_RF, Y_preds_T2T])\n",
    "    Y_tests_pairs.append(Y_tests)\n",
    "    scores_per_pair.append(scores_per_DA)\n",
    "    errorbars_per_pair.append(errorbars)\n",
    "    feature_importance_per_pair.append([F_importance_GFK, F_importance_RF, F_importance_T2T])\n",
    "    \n",
    "mlab.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------Plot the scores for each class pair using the four methods above----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump([scores_per_pair, errorbars_per_pair, feature_importance_per_pair], open('GFK_CODA_RF_pair_1_2_balanced_source.p', \"w\" ))\n",
    "\n",
    "n_methods = 4\n",
    "n_pairs = 1\n",
    "bar_width = 0.35\n",
    "gap = 1\n",
    "index = np.arange(0, (bar_width*n_methods+gap)*n_pairs, bar_width*n_methods+gap)\n",
    "opacity = 0.8\n",
    "error_config = {'ecolor': '0.3'}\n",
    "colors = ['b','g','r','c']\n",
    "fig, ax = plt.subplots(figsize=(18.75,10))\n",
    "for i in range(0,n_methods):\n",
    "    rects1 = plt.bar(index+i*bar_width, temp_score[:,i], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color=colors[i],\n",
    "                     yerr=temp_error[:,i],\n",
    "                     error_kw=error_config,\n",
    "                     label=labels[i])\n",
    "    \n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 0.4), fontsize=16)\n",
    "\n",
    "plt.title('Comparison of different Domain Adaptation techniques', fontsize=20)\n",
    "plt.xlabel('Class pairs', fontsize=16)\n",
    "plt.ylabel('Average accuracy', fontsize=16)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "plt.xticks(index + n_methods*bar_width/2, Selected_pairs)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
